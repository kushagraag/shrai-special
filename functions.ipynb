{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRROA3Kp-xFS"
      },
      "source": [
        "#Link Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC4d4RZd-fTB"
      },
      "source": [
        "## Link parsing small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kK8MUEriuxMT"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# working with initial 10 links. successfully takes required links and save them in a file\n",
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "from requests.exceptions import Timeout  # Import the Timeout exception for links which takes too much time to load\n",
        "\n",
        "# Function to categorize links on a webpage\n",
        "def categorize_links(links):\n",
        "    maps_link = \"\"\n",
        "    instagram_link = \"\"\n",
        "    facebook_link = \"\"\n",
        "    twitter_link = \"\"\n",
        "    other_links = []\n",
        "\n",
        "    for link in links:\n",
        "        if \"maps.google.com\" in link:\n",
        "            maps_link = link\n",
        "        elif \"www.instagram.com\" in link:\n",
        "            instagram_link = link\n",
        "        elif \"www.facebook.com\" in link:\n",
        "            facebook_link = link\n",
        "        elif \"twitter.com\" in link:\n",
        "            twitter_link = link\n",
        "        else:\n",
        "            other_links.append(link)\n",
        "\n",
        "    return [maps_link, instagram_link, facebook_link, twitter_link] + other_links\n",
        "\n",
        "\n",
        "def extract_links_with_error_handling(url, index):\n",
        "    retries = 3  # Number of retries before giving up\n",
        "    retry_delay = 2  # Delay between retries in seconds\n",
        "    for _ in range(retries):\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)  # Set a timeout of 10 seconds\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            links = [a['href'] for a in soup.find_all('a', href=True)]\n",
        "            valid_links = [link for link in links if re.match(r'^https?://', link)]\n",
        "            return list(set(valid_links))  # Remove duplicates by converting to a set and back to a list\n",
        "        except Timeout:\n",
        "            print(f\"Timeout occurred for index {index} and URL {url}. Retrying...\")\n",
        "            time.sleep(retry_delay)\n",
        "        except ConnectionError:\n",
        "            print(f\"Connection error occurred for index {index} and URL {url}. Retrying...\")\n",
        "            time.sleep(retry_delay)\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting links from index {index} for URL {url}: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "# Initialize an empty dictionary to store the data\n",
        "link_data = {}\n",
        "\n",
        "# Read the CSV file containing links\n",
        "csv_filename = 'outfile_small.csv'  # Change this to your CSV file name\n",
        "with open(csv_filename, 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "\n",
        "    # Iterate over each row in the CSV file\n",
        "    for index, row in enumerate(reader):\n",
        "        if len(row) < 2:\n",
        "            continue  # Skip rows with less than 2 columns\n",
        "\n",
        "        # Assuming the link is in the second column and the index is in the first column\n",
        "        link_index = row[0]\n",
        "        link = row[1]\n",
        "        if(link == ''):\n",
        "            continue\n",
        "        print('going for ', link_index, ' : ', link)\n",
        "        # Extract links from the current URL and store them as values for the key (current link)\n",
        "        link_data[link] = extract_links(link, link_index)\n",
        "\n",
        "        # Introduce a delay of 2 seconds before processing the next link\n",
        "        time.sleep(1)  # You can adjust the sleep duration as needed\n",
        "\n",
        "# Categorize links after extracting all links\n",
        "for key, values in link_data.items():\n",
        "    link_data[key] = categorize_links(values)\n",
        "\n",
        "# Print the resulting dictionary and save it to an output file\n",
        "output_filename = 'link_data.csv'\n",
        "with open(output_filename, 'w', newline='') as output_csv:\n",
        "    writer = csv.writer(output_csv)\n",
        "    for key, values in link_data.items():\n",
        "        print(f\"Link: {key}\")\n",
        "        print(\"Links found on the page:\")\n",
        "        writer.writerow([key] + values)\n",
        "        for value in values:\n",
        "            print(f\" - {value}\")\n",
        "        print()\n",
        "\n",
        "print(f\"Data saved to {output_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB6j7vm3-lPS"
      },
      "source": [
        "##Link Parsing large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBp3ozmFuxMU"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# works with 50 links without issues. also handles most exceptions\n",
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "from requests.exceptions import Timeout, RequestException  # Import the Timeout exception for links which takes too much time to load\n",
        "\n",
        "# Function to categorize links on a webpage\n",
        "def categorize_links(links):\n",
        "    if links is None:\n",
        "        return []\n",
        "    maps_link = \"\"\n",
        "    instagram_link = \"\"\n",
        "    facebook_link = \"\"\n",
        "    twitter_link = \"\"\n",
        "    other_links = []\n",
        "\n",
        "    for link in links:\n",
        "        if \"maps.google.com\" in link:\n",
        "            maps_link = link\n",
        "        elif \"www.instagram.com\" in link:\n",
        "            instagram_link = link\n",
        "        elif \"www.facebook.com\" in link:\n",
        "            facebook_link = link\n",
        "        elif \"twitter.com\" in link:\n",
        "            twitter_link = link\n",
        "        else:\n",
        "            other_links.append(link)\n",
        "\n",
        "    return [maps_link, instagram_link, facebook_link, twitter_link] + other_links\n",
        "\n",
        "\n",
        "def extract_links_with_error_handling(url, index):\n",
        "    retries = 3  # Number of retries before giving up\n",
        "    retry_delay = 2  # Delay between retries in seconds\n",
        "    for _ in range(retries):\n",
        "        try:\n",
        "            print('heyaaaa')\n",
        "            response = requests.get(url, timeout=(5, 5))  # Set a timeout of 10 seconds for both connect and read\n",
        "            print('aaaayeh')\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            links = [a['href'] for a in soup.find_all('a', href=True)]\n",
        "            valid_links = [link for link in links if re.match(r'^https?://', link)]\n",
        "            return list(set(valid_links))  # Remove duplicates by converting to a set and back to a list\n",
        "        except Timeout:\n",
        "            print(f\"Timeout occurred for index {index} and URL {url}. Retrying...\")\n",
        "            time.sleep(retry_delay)\n",
        "        except RequestException as e:\n",
        "            print(f\"RequestException occurred for index {index} and URL {url}: {str(e)}\")\n",
        "            return []  # Exit the loop and move to the next link\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting links from index {index} for URL {url}: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "# Initialize an empty dictionary to store the data\n",
        "link_data = {}\n",
        "\n",
        "# Read the CSV file containing links\n",
        "csv_filename = 'outfile.csv'  # Change this to your CSV file name\n",
        "with open(csv_filename, 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "\n",
        "    # Iterate over each row in the CSV file\n",
        "    for index, row in enumerate(reader):\n",
        "        if len(row) < 2:\n",
        "            continue  # Skip rows with less than 2 columns\n",
        "\n",
        "        # Assuming the link is in the second column and the index is in the first column\n",
        "        link_index = row[0]\n",
        "        link = row[1]\n",
        "        if(link == ''):\n",
        "            continue\n",
        "        print('going for ', link_index, ' : ', link)\n",
        "        # Extract links from the current URL and store them as values for the key (current link)\n",
        "        link_data[link] = extract_links_with_error_handling(link, link_index)\n",
        "\n",
        "        # Introduce a delay of 2 seconds before processing the next link\n",
        "        time.sleep(1)  # You can adjust the sleep duration as needed\n",
        "\n",
        "# Categorize links after extracting all links\n",
        "for key, values in link_data.items():\n",
        "    link_data[key] = categorize_links(values)\n",
        "\n",
        "# Print the resulting dictionary and save it to an output file\n",
        "output_filename = 'link_data_full.csv'\n",
        "with open(output_filename, 'w', newline='') as output_csv:\n",
        "    writer = csv.writer(output_csv)\n",
        "    for key, values in link_data.items():\n",
        "        print(f\"Link: {key}\")\n",
        "        print(\"Links found on the page:\")\n",
        "        writer.writerow([key] + values)\n",
        "        for value in values:\n",
        "            print(f\" - {value}\")\n",
        "        print()\n",
        "\n",
        "print(f\"Data saved to {output_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYm6au97ZwRl"
      },
      "source": [
        "## links -> deep traversals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALN-ljC4aFBe"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# works with 50 links without issues. also handles most exceptions\n",
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "from requests.exceptions import Timeout, RequestException  # Import the Timeout exception for links which takes too much time to load\n",
        "\n",
        "# Function to categorize links on a webpage\n",
        "def categorize_links(links):\n",
        "    if links is None:\n",
        "        return []\n",
        "    maps_link = \"\"\n",
        "    instagram_link = \"\"\n",
        "    facebook_link = \"\"\n",
        "    twitter_link = \"\"\n",
        "    other_links = []\n",
        "\n",
        "    for link in links:\n",
        "        if \"maps.google.com\" in link:\n",
        "            maps_link = link\n",
        "        elif \"www.instagram.com\" in link:\n",
        "            instagram_link = link\n",
        "        elif \"www.facebook.com\" in link:\n",
        "            facebook_link = link\n",
        "        elif \"twitter.com\" in link:\n",
        "            twitter_link = link\n",
        "        else:\n",
        "            other_links.append(link)\n",
        "\n",
        "    return [maps_link, instagram_link, facebook_link, twitter_link] + other_links\n",
        "\n",
        "\n",
        "def extract_links_with_error_handling(url, index):\n",
        "    retries = 3  # Number of retries before giving up\n",
        "    retry_delay = 2  # Delay between retries in seconds\n",
        "    for _ in range(retries):\n",
        "        try:\n",
        "            print('heyaaaa')\n",
        "            response = requests.get(url, timeout=(5, 5))  # Set a timeout of 10 seconds for both connect and read\n",
        "            print('aaaayeh')\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            links = [a['href'] for a in soup.find_all('a', href=True)]\n",
        "            valid_links = [link for link in links if re.match(r'^https?://', link)]\n",
        "            return list(set(valid_links))  # Remove duplicates by converting to a set and back to a list\n",
        "        except Timeout:\n",
        "            print(f\"Timeout occurred for index {index} and URL {url}. Retrying...\")\n",
        "            time.sleep(retry_delay)\n",
        "        except RequestException as e:\n",
        "            print(f\"RequestException occurred for index {index} and URL {url}: {str(e)}\")\n",
        "            return []  # Exit the loop and move to the next link\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting links from index {index} for URL {url}: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "# Initialize an empty dictionary to store the data\n",
        "link_data = {}\n",
        "\n",
        "# Read the CSV file containing links\n",
        "csv_filename = 'outfile.csv'  # Change this to your CSV file name\n",
        "with open(csv_filename, 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "\n",
        "    # Iterate over each row in the CSV file\n",
        "    for index, row in enumerate(reader):\n",
        "        if len(row) < 2:\n",
        "            continue  # Skip rows with less than 2 columns\n",
        "\n",
        "        # Assuming the link is in the second column and the index is in the first column\n",
        "        link_index = row[0]\n",
        "        link = row[1]\n",
        "        if(link == ''):\n",
        "            continue\n",
        "        print('going for ', link_index, ' : ', link)\n",
        "        # Extract links from the current URL and store them as values for the key (current link)\n",
        "        link_data[link] = extract_links_with_error_handling(link, link_index)\n",
        "\n",
        "        # Introduce a delay of 2 seconds before processing the next link\n",
        "        time.sleep(1)  # You can adjust the sleep duration as needed\n",
        "\n",
        "# Categorize links after extracting all links\n",
        "for key, values in link_data.items():\n",
        "    link_data[key] = categorize_links(values)\n",
        "\n",
        "# Print the resulting dictionary and save it to an output file\n",
        "output_filename = 'link_data_full.csv'\n",
        "with open(output_filename, 'w', newline='') as output_csv:\n",
        "    writer = csv.writer(output_csv)\n",
        "    for key, values in link_data.items():\n",
        "        print(f\"Link: {key}\")\n",
        "        print(\"Links found on the page:\")\n",
        "        writer.writerow([key] + values)\n",
        "        for value in values:\n",
        "            print(f\" - {value}\")\n",
        "        print()\n",
        "\n",
        "print(f\"Data saved to {output_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq2YrK_HZgxH"
      },
      "source": [
        "# regex codes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPW7Awa3-qxb"
      },
      "source": [
        "## trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FiI2HR3_05p"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Sample HTML content (replace this with your actual HTML content)\n",
        "html_content = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Sample Page</title>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"header\" id=\"top-header\">\n",
        "        <h1>Contact Us</h1>\n",
        "        <p>Phone: +1 (123) 123-1223</p>\n",
        "        <p>Email: info@example.com</p>\n",
        "    </div>\n",
        "    <div class=\"content\">\n",
        "        <p class=\"text\">Visit our site for more information.</p>\n",
        "    </div>\n",
        "    <footer class=\"footer\" id=\"page-footer\">\n",
        "        <p>&copy; 2023 Sample Company</p>\n",
        "    </footer>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Parse the HTML content using Beautiful Soup\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Define a regex pattern to match phone numbers\n",
        "phone_pattern = r'(\\+\\d{1,2}\\s?)?\\(\\d{3}\\)\\s?\\d{3}[-\\s]\\d{4}'\n",
        "\n",
        "# Find elements containing phone numbers and capture their class attributes\n",
        "phone_number_elements = soup.find_all(text=re.compile(phone_pattern))\n",
        "\n",
        "for element in phone_number_elements:\n",
        "    # Find the nearest parent element with a class attribute\n",
        "    parent_element = element.find_parent(attrs={\"class\": True})\n",
        "    element_class = parent_element['class'] if parent_element else None\n",
        "\n",
        "    phone_number = element.strip()\n",
        "\n",
        "    # Check if phone_number is not None before calling strip()\n",
        "    if phone_number is not None:\n",
        "        phone_number = phone_number.strip()\n",
        "\n",
        "    print(f\"Phone Number: {phone_number}\")\n",
        "    print(f\"Class Attribute: {element_class}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlPvbU56FMn0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Sample HTML content (replace this with your actual HTML content)\n",
        "html_content = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Complex Page</title>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"header\" id=\"top-header\">\n",
        "        <h1>Contact Us</h1>\n",
        "        <div class=\"contact-info\">\n",
        "            <p class=\"phone\">Phone: +1 (123) 123-1223</p>\n",
        "            <p class=\"email\">Email: info@example.com</p>\n",
        "        </div>\n",
        "        <div class=\"address\">\n",
        "            <p>Main Office:</p>\n",
        "            <p class=\"street\">123 Main St</p>\n",
        "            <p class=\"city\">Cityville</p>\n",
        "            <p class=\"country\">Countryland</p>\n",
        "        </div>\n",
        "    </div>\n",
        "    <div class=\"content\">\n",
        "        <h2>About Us</h2>\n",
        "        <p class=\"description\">We are a company specializing in...</p>\n",
        "        <div class=\"links\">\n",
        "            <a href=\"https://www.facebook.com\" class=\"social-link\">Facebook</a>\n",
        "            <a href=\"https://www.twitter.com\" class=\"social-link\">Twitter</a>\n",
        "            <a href=\"https://www.instagram.com\" class=\"social-link\">Instagram</a>\n",
        "        </div>\n",
        "    </div>\n",
        "    <footer class=\"footer\" id=\"page-footer\">\n",
        "        <div class=\"opening-hours\">\n",
        "            <p class=\"day\">Monday - Friday</p>\n",
        "            <p class=\"hours\">9 AM - 6 PM</p>\n",
        "        </div>\n",
        "        <div class=\"copyright\">\n",
        "            <p>&copy; 2023 Sample Company</p>\n",
        "        </div>\n",
        "    </footer>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Parse the HTML content using Beautiful Soup\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Define regex patterns for phone num\\bgroup\\sorder\\bbers, email addresses, timings, and addresses\n",
        "phone_pattern = r'(\\+\\d{1,2}\\s?)?\\(\\d{3}\\)\\s?\\d{3}[-\\s]\\d{4}'\n",
        "email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
        "timings_pattern = r'(?:Mon|Tue|Wed|Thu|Fri|Sat|Sun)(?:day)? [0-9]+(?: AM| PM)? - [0-9]+(?: AM| PM)?'\n",
        "address_pattern = r'\\b\\d+\\s[A-Za-z\\s,]+\\b'\n",
        "catering_pattern = re.compile(r'\\bcatering\\b', re.IGNORECASE)\n",
        "group_order_pattern = re.compile(r'\\bgroup\\sorder\\b', re.IGNORECASE)\n",
        "counter_pattern = re.compile(r'\\bcounter\\b', re.IGNORECASE)\n",
        "delivery_pattern = re.compile(r'\\bdelivery\\b', re.IGNORECASE)\n",
        "bar_pattern = re.compile(r'\\bbar\\b', re.IGNORECASE)\n",
        "pickup_pattern = re.compile(r'\\bpick\\s*-\\s*up\\b', re.IGNORECASE)\n",
        "\n",
        "\n",
        "# Find elements containing phone numbers, email addresses, timings, and addresses and capture their class attributes\n",
        "phone_number_elements = soup.find_all(text=re.compile(phone_pattern))\n",
        "email_elements = soup.find_all(text=re.compile(email_pattern))\n",
        "timings_elements = soup.find_all(text=re.compile(timings_pattern))\n",
        "address_elements = soup.find_all(text=re.compile(address_pattern))\n",
        "\n",
        "for element in phone_number_elements:\n",
        "    # Find the nearest parent element with a class attribute\n",
        "    parent_element = element.find_parent(attrs={\"class\": True})\n",
        "    element_class = parent_element['class'] if parent_element else None\n",
        "\n",
        "    phone_number = element.strip()\n",
        "\n",
        "    # Check if phone_number is not None before calling strip()\n",
        "    if phone_number is not None:\n",
        "        phone_number = phone_number.strip()\n",
        "\n",
        "    print(f\"Phone Number: {phone_number}\")\n",
        "    print(f\"Class Attribute: {element_class}\")\n",
        "\n",
        "for element in email_elements:\n",
        "    # Find the nearest parent element with a class attribute\n",
        "    parent_element = element.find_parent(attrs={\"class\": True})\n",
        "    element_class = parent_element['class'] if parent_element else None\n",
        "\n",
        "    email = element.strip()\n",
        "\n",
        "    # Check if email is not None before calling strip()\n",
        "    if email is not None:\n",
        "        email = email.strip()\n",
        "\n",
        "    print(f\"Email Address: {email}\")\n",
        "    print(f\"Class Attribute: {element_class}\")\n",
        "\n",
        "for element in timings_elements:\n",
        "    # Find the nearest parent element with a class attribute\n",
        "    parent_element = element.find_parent(attrs={\"class\": True})\n",
        "    element_class = parent_element['class'] if parent_element else None\n",
        "\n",
        "    timings = element.strip()\n",
        "\n",
        "    # Check if timings is not None before calling strip()\n",
        "    if timings is not None:\n",
        "        timings = timings.strip()\n",
        "\n",
        "    print(f\"Timings: {timings}\")\n",
        "    print(f\"Class Attribute: {element_class}\")\n",
        "\n",
        "for element in address_elements:\n",
        "    # Find the nearest parent element with a class attribute\n",
        "    parent_element = element.find_parent(attrs={\"class\": True})\n",
        "    element_class = parent_element['class'] if parent_element else None\n",
        "\n",
        "    address = element.strip()\n",
        "\n",
        "    # Check if address is not None before calling strip()\n",
        "    if address is not None:\n",
        "        address = address.strip()\n",
        "\n",
        "    print(f\"Address: {address}\")\n",
        "    print(f\"Class Attribute: {element_class}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_63qgClJYaBE"
      },
      "source": [
        "links traverse and web crawling with threading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPP8KyaYZOwZ"
      },
      "source": [
        "## same as below, without functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dlYnPtTiWbB"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Sample HTML content (replace this with your actual HTML content)\n",
        "html_content = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Complex Page</title>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"header\" id=\"top-header\">\n",
        "        <h1>Contact Us</h1>\n",
        "        <div class=\"contact-info\">\n",
        "            <p class=\"phone\">Phone: +1 (123) 123-1223</p>\n",
        "            <p class=\"email\">Email: info@example.com</p>\n",
        "        </div>\n",
        "        <div class=\"address\">\n",
        "            <p>Main Office:</p>\n",
        "            <p class=\"street\">123 Main St</p>\n",
        "            <p class=\"city\">Cityville</p>\n",
        "            <p class=\"country\">Countryland</p>\n",
        "        </div>\n",
        "    </div>\n",
        "    <div class=\"content\">\n",
        "        <h2>About Us</h2>\n",
        "        <p class=\"description\">We are a company specializing in...</p>\n",
        "        <div class=\"links\">\n",
        "            <a href=\"https://www.facebook.com\" class=\"social-link\">Facebook</a>\n",
        "            <a href=\"https://www.twitter.com\" class=\"social-link\">Twitter</a>\n",
        "            <a href=\"https://www.instagram.com\" class=\"social-link\">Instagram</a>\n",
        "        </div>\n",
        "    </div>\n",
        "    <footer class=\"footer\" id=\"page-footer\">\n",
        "        <div class=\"opening-hours\">\n",
        "            <p class=\"day\">Monday - Friday</p>\n",
        "            <p class=\"hours\">9 AM - 6 PM</p>\n",
        "        </div>\n",
        "        <div class=\"copyright\">\n",
        "            <p>&copy; 2023 Sample Company</p>\n",
        "        </div>\n",
        "    </footer>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Parse the HTML content using Beautiful Soup\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Define regex patterns for phone num\\bgroup\\sorder\\bbers, email addresses, timings, and addresses\n",
        "phone_pattern = r'(\\+\\d{1,2}\\s?)?\\(\\d{3}\\)\\s?\\d{3}[-\\s]\\d{4}'\n",
        "email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
        "timings_pattern = r'(?:Mon|Tue|Wed|Thu|Fri|Sat|Sun)(?:day)? [0-9]+(?: AM| PM)? - [0-9]+(?: AM| PM)?'\n",
        "address_pattern = r'\\b\\d+\\s[A-Za-z\\s,]+\\b'\n",
        "catering_pattern = re.compile(r'\\bcatering\\b', re.IGNORECASE)\n",
        "group_order_pattern = re.compile(r'\\bgroup\\sorder\\b', re.IGNORECASE)\n",
        "counter_pattern = re.compile(r'\\bcounter\\b', re.IGNORECASE)\n",
        "delivery_pattern = re.compile(r'\\bdelivery\\b', re.IGNORECASE)\n",
        "bar_pattern = re.compile(r'\\bbar\\b', re.IGNORECASE)\n",
        "pickup_pattern = re.compile(r'\\bpick\\s*-\\s*up\\b', re.IGNORECASE)\n",
        "\n",
        "\n",
        "# Find elements containing phone numbers, email addresses, timings, and addresses and capture their class attributes\n",
        "phone_number_elements = soup.find_all(text=re.compile(phone_pattern))\n",
        "email_elements = soup.find_all(text=re.compile(email_pattern))\n",
        "timings_elements = soup.find_all(text=re.compile(timings_pattern))\n",
        "address_elements = soup.find_all(text=re.compile(address_pattern))\n",
        "\n",
        "for element in phone_number_elements:\n",
        "    # Find the nearest parent element with a class attribute\n",
        "    parent_element = element.find_parent(attrs={\"class\": True})\n",
        "    element_class = parent_element['class'] if parent_element else None\n",
        "\n",
        "    phone_number = element.strip()\n",
        "\n",
        "    # Check if phone_number is not None before calling strip()\n",
        "    if phone_number is not None:\n",
        "        phone_number = phone_number.strip()\n",
        "\n",
        "    print(f\"Phone Number: {phone_number}\")\n",
        "    print(f\"Class Attribute: {element_class}\")\n",
        "\n",
        "for element in email_elements:\n",
        "    # Find the nearest parent element with a class attribute\n",
        "    parent_element = element.find_parent(attrs={\"class\": True})\n",
        "    element_class = parent_element['class'] if parent_element else None\n",
        "\n",
        "    email = element.strip()\n",
        "\n",
        "    # Check if email is not None before calling strip()\n",
        "    if email is not None:\n",
        "        email = email.strip()\n",
        "\n",
        "    print(f\"Email Address: {email}\")\n",
        "    print(f\"Class Attribute: {element_class}\")\n",
        "\n",
        "for element in timings_elements:\n",
        "    # Find the nearest parent element with a class attribute\n",
        "    parent_element = element.find_parent(attrs={\"class\": True})\n",
        "    element_class = parent_element['class'] if parent_element else None\n",
        "\n",
        "    timings = element.strip()\n",
        "\n",
        "    # Check if timings is not None before calling strip()\n",
        "    if timings is not None:\n",
        "        timings = timings.strip()\n",
        "\n",
        "    print(f\"Timings: {timings}\")\n",
        "    print(f\"Class Attribute: {element_class}\")\n",
        "\n",
        "for element in address_elements:\n",
        "    # Find the nearest parent element with a class attribute\n",
        "    parent_element = element.find_parent(attrs={\"class\": True})\n",
        "    element_class = parent_element['class'] if parent_element else None\n",
        "\n",
        "    address = element.strip()\n",
        "\n",
        "    # Check if address is not None before calling strip()\n",
        "    if address is not None:\n",
        "        address = address.strip()\n",
        "\n",
        "    print(f\"Address: {address}\")\n",
        "    print(f\"Class Attribute: {element_class}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga1TFchoCvXQ"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#  links traverse and web crawling with threading\n",
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "from requests.exceptions import Timeout, RequestException\n",
        "import threading\n",
        "\n",
        "# Function to categorize links on a webpage\n",
        "def categorize_links(links):\n",
        "    if links is None:\n",
        "        return []\n",
        "    maps_link = \"\"\n",
        "    instagram_link = \"\"\n",
        "    facebook_link = \"\"\n",
        "    twitter_link = \"\"\n",
        "    other_links = []\n",
        "\n",
        "    for link in links:\n",
        "        if \"maps.google.com\" in link:\n",
        "            maps_link = link\n",
        "        elif \"www.instagram.com\" in link:\n",
        "            instagram_link = link\n",
        "        elif \"www.facebook.com\" in link:\n",
        "            facebook_link = link\n",
        "        elif \"twitter.com\" in link:\n",
        "            twitter_link = link\n",
        "        else:\n",
        "            other_links.append(link)\n",
        "\n",
        "    return [maps_link, instagram_link, facebook_link, twitter_link] + other_links\n",
        "\n",
        "def extract_links_with_error_handling(url, index):\n",
        "    retries = 3\n",
        "    retry_delay = 2\n",
        "    for _ in range(retries):\n",
        "        try:\n",
        "            print('heyaaaa')\n",
        "            response = requests.get(url, timeout=(5, 5))\n",
        "            print('aaaayeh')\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            links = [a['href'] for a in soup.find_all('a', href=True)]\n",
        "            valid_links = [link for link in links if re.match(r'^https?://', link)]\n",
        "            return list(set(valid_links))\n",
        "        except Timeout:\n",
        "            print(f\"Timeout occurred for index {index} and URL {url}. Retrying...\")\n",
        "            time.sleep(retry_delay)\n",
        "        except RequestException as e:\n",
        "            print(f\"RequestException occurred for index {index} and URL {url}: {str(e)}\")\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting links from index {index} for URL {url}: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "# Function for multithreading\n",
        "def process_url(index, link):\n",
        "    print('going for ', index, ' : ', link)\n",
        "    link_data[link] = extract_links_with_error_handling(link, index)\n",
        "    time.sleep(1)\n",
        "\n",
        "# Initialize an empty dictionary to store the data\n",
        "link_data = {}\n",
        "\n",
        "# Read the CSV file containing links\n",
        "csv_filename = 'outfile.csv'  # Change this to your CSV file name\n",
        "with open(csv_filename, 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "\n",
        "    # Iterate over each row in the CSV file\n",
        "    threads = []\n",
        "    for index, row in enumerate(reader):\n",
        "        if len(row) < 2:\n",
        "            continue\n",
        "\n",
        "        # Assuming the link is in the second column and the index is in the first column\n",
        "        link_index = row[0]\n",
        "        link = row[1]\n",
        "        if(link == ''):\n",
        "            continue\n",
        "\n",
        "        # Create a thread to process the URL\n",
        "        thread = threading.Thread(target=process_url, args=(link_index, link))\n",
        "        threads.append(thread)\n",
        "\n",
        "    # Start all threads\n",
        "    for thread in threads:\n",
        "        thread.start()\n",
        "\n",
        "    # Wait for all threads to finish\n",
        "    for thread in threads:\n",
        "        thread.join()\n",
        "\n",
        "# Categorize links after extracting all links\n",
        "for key, values in link_data.items():\n",
        "    link_data[key] = categorize_links(values)\n",
        "\n",
        "# Print the resulting dictionary and save it to an output file\n",
        "output_filename = 'link_data_full.csv'\n",
        "with open(output_filename, 'w', newline='') as output_csv:\n",
        "    writer = csv.writer(output_csv)\n",
        "    for key, values in link_data.items():\n",
        "        print(f\"Link: {key}\")\n",
        "        print(\"Links found on the page:\")\n",
        "        writer.writerow([key] + values)\n",
        "        for value in values:\n",
        "            print(f\" - {value}\")\n",
        "        print()\n",
        "\n",
        "print(f\"Data saved to {output_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNj8_85WZHig"
      },
      "source": [
        "## dummy code for all below codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "J2c-UnSlSzal",
        "outputId": "e18d5ac1-2b15-4d81-80d5-2447cb54511d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-518d7863a587>:45: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  elements = soup.find_all(text=re.compile(pattern, re.IGNORECASE))\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://maps.google.com/?q=3583+16th+St,+San+Francisco,+CA+94114,+USA&ftid=0x808f7e1c89dc2e5b:0xa1d14effd3552b22'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Sample HTML content\n",
        "html_content = \"\"\"\n",
        "<html>\n",
        "<head>\n",
        "    <title>Sample Page</title>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"header\" id=\"top-header\">\n",
        "        <h1>Contact Us</h1>\n",
        "        <p>Phone: +1 (123) 123-1223</p>\n",
        "        <p>Email: info@example.com</p>\n",
        "    </div>\n",
        "    <div class=\"content\">\n",
        "        <p class=\"text\">Visit our site for more information.</p>\n",
        "    </div>\n",
        "    <div>\n",
        "    <a href=\"https://maps.google.com/?q=3583+16th+St,+San+Francisco,+CA+94114,+USA&amp;ftid=0x808f7e1c89dc2e5b:0xa1d14effd3552b22\" class=\"site-location__address\" target=\"_blank\" rel=\"noopener\" data-bb-track=\"button\" data-bb-track-on=\"click\" data-bb-track-category=\"Address\" data-bb-track-action=\"Click\" data-bb-track-label=\"Header\">\n",
        "    <span>3583 16th St,</span>\n",
        "    <span> San Francisco, CA 94114</span></a>\n",
        "    </div>\n",
        "    <footer class=\"footer\" id=\"page-footer\">\n",
        "        <p>&copy; 2023 Sample Company</p>\n",
        "    </footer>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "phone_pattern = r'(\\+\\d{1,2}\\s?)?\\(\\d{3}\\)\\s?\\d{3}[-\\s]\\d{4}'\n",
        "email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
        "timings_pattern = r'(?:Mon|Tue|Wed|Thu|Fri|Sat|Sun)(?:day)? [0-9]+(?: AM| PM)? - [0-9]+(?: AM| PM)?'\n",
        "# address_pattern = r'\\b\\d+\\s[A-Za-z\\s,]+\\b'\n",
        "catering_pattern = re.compile(r'\\bcatering\\b', re.IGNORECASE)\n",
        "group_order_pattern = re.compile(r'\\bgroup\\sorder\\b', re.IGNORECASE)\n",
        "counter_pattern = re.compile(r'\\bcounter\\b', re.IGNORECASE)\n",
        "delivery_pattern = re.compile(r'\\bdelivery\\b', re.IGNORECASE)\n",
        "bar_pattern = re.compile(r'\\bbar\\b', re.IGNORECASE)\n",
        "pickup_pattern = re.compile(r'\\bpick\\s*-\\s*up\\b', re.IGNORECASE)\n",
        "\n",
        "address_pattern = r'\\baddress\\b'\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "def get_data_by_regex(pattern):\n",
        "  elements = soup.find_all(text=re.compile(pattern, re.IGNORECASE))\n",
        "  for element in elements:\n",
        "    # Find the nearest parent element with a class attribute\n",
        "    parent_element = element.find_parent(attrs={\"class\": True})\n",
        "    elements_with_matching_attributes = soup.find_all(attrs={'data-custom': pattern})\n",
        "    element_class = parent_element['class'] if parent_element else None\n",
        "    print(\"is it even working\")\n",
        "    print(elements_with_matching_attributes)\n",
        "    print(\"hello\")\n",
        "    address = element.strip()\n",
        "\n",
        "    # Check if address is not None before calling strip()\n",
        "    if address is not None:\n",
        "        address = address.strip()\n",
        "    if element_class == None:\n",
        "      return address\n",
        "    else:\n",
        "      address=get_class_data(element_class)\n",
        "      return address\n",
        "\n",
        "def get_class_data(address):\n",
        "  data=[]\n",
        "  header_element = soup.find_all(class_=address)\n",
        "  print(header_element)\n",
        "\n",
        "  if header_element:\n",
        "    for header in header_element:\n",
        "    # Extract the text content of the element\n",
        "      header_text = header.get_text()\n",
        "      data.append(header_text)\n",
        "  return data\n",
        "ad=get_data_by_regex(address_pattern)\n",
        "\n",
        "# Alternatively, if you want to get the HTML content:\n",
        "# header_html = str(header_element)\n",
        "# print(\"Header HTML:\")\n",
        "# print(header_html)\n",
        "\"https://maps.google.com/?q=3583+16th+St,+San+Francisco,+CA+94114,+USA&ftid=0x808f7e1c89dc2e5b:0xa1d14effd3552b22\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCCJTxOB-JL6"
      },
      "source": [
        "## ADDRESS complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP7tApSa7fih",
        "outputId": "fa8428eb-5cc0-4404-9389-9a345c454da2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2135 Franklin St.\n",
            "Class Attribute: ['wixui-rich-text__text']\n",
            "Â© 2023 Sample Company\n",
            "Class Attribute: ['footer']\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-31-7f1567d2b5d1>:61: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  address_elements = soup.find_all(text=re.compile(address_pattern))\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Sample HTML content (replace this with your actual HTML content)\n",
        "html_content = \"\"\"\n",
        "<html>\n",
        "<head>\n",
        "    <title>Sample Page</title>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"header\" id=\"top-header\">\n",
        "        <h1>Contact Us</h1>\n",
        "        <p>Phone: +1 (123) 123-1223</p>\n",
        "        <p>Email: info@example.com</p>\n",
        "    </div>\n",
        "    <div class=\"content\">\n",
        "        <p class=\"text\">Visit our site for more information.</p>\n",
        "    </div>\n",
        "    <div data-mesh-id=\"comp-l48anq6vinlineContent-gridContainer\" data-testid=\"mesh-container-content\"><div id=\"comp-l39r6gmj\" class=\"comp-l39r6gmj wixui-vector-image\" data-angle=\"0\" data-angle-style-location=\"style\" style=\"visibility: inherit;\" data-screen-in-hide=\"done\"><div data-testid=\"svgRoot-comp-l39r6gmj\" class=\"aizuI7 TcoJIb comp-l39r6gmj\"><svg preserveAspectRatio=\"xMidYMid meet\" data-bbox=\"53.5 36.5 93 127\" viewBox=\"53.5 36.5 93 127\" height=\"200\" width=\"200\" xmlns=\"http://www.w3.org/2000/svg\" data-type=\"color\" role=\"presentation\" aria-hidden=\"true\" aria-label=\"\"><defs><style>#comp-l39r6gmj svg [data-color=\"1\"] {fill: #E02728;}</style></defs>\n",
        "    <g>\n",
        "        <path d=\"M99.999 163.5l-3.25-3.895C94.986 157.487 53.5 107.468 53.5 82.916 53.5 57.323 74.359 36.5 99.999 36.5c25.644 0 46.501 20.823 46.501 46.416 0 24.551-41.483 74.571-43.252 76.688l-3.249 3.896zm0-118.56c-20.978 0-38.046 17.036-38.046 37.977 0 16.359 25.019 51.015 38.046 67.305 13.029-16.29 38.048-50.946 38.048-67.305 0-20.942-17.068-37.977-38.048-37.977z\" fill=\"#2F54DD\" data-color=\"1\"></path>\n",
        "        <path d=\"M99.999 101.658c-10.351 0-18.775-8.407-18.775-18.741 0-10.335 8.424-18.743 18.775-18.743 10.353 0 18.777 8.408 18.777 18.743 0 10.333-8.424 18.741-18.777 18.741zm0-29.046c-5.69 0-10.32 4.621-10.32 10.304 0 5.68 4.63 10.303 10.32 10.303 5.692 0 10.324-4.622 10.324-10.303 0-5.682-4.632-10.304-10.324-10.304z\" fill=\"#2F54DD\" data-color=\"1\"></path>\n",
        "    </g>\n",
        "</svg>\n",
        "</div></div><div id=\"comp-l39r6gnw\" class=\"KcpHeO tz5f0K comp-l39r6gnw wixui-rich-text\" data-testid=\"richTextElement\" data-angle=\"0\" data-angle-style-location=\"style\" style=\"visibility: inherit;\" data-screen-in-hide=\"done\"><p class=\"font_5 wixui-rich-text__text\" style=\"font-size:20px;\"><span style=\"font-size:20px;\" class=\"wixui-rich-text__text\">Address</span></p></div><div id=\"comp-l39r6go3\" class=\"KcpHeO tz5f0K comp-l39r6go3 wixui-rich-text\" data-testid=\"richTextElement\" data-angle=\"0\" data-angle-style-location=\"style\" style=\"visibility: inherit;\" data-screen-in-hide=\"done\"><p class=\"font_8 wixui-rich-text__text\" style=\"font-size:16px; line-height:1.8em;\"><span style=\"font-size:16px;\" class=\"wixui-rich-text__text\">2135 Franklin St.</span></p>\n",
        "\n",
        "<p class=\"font_8 wixui-rich-text__text\" style=\"font-size:16px; line-height:1.8em;\"><span style=\"font-size:16px;\" class=\"wixui-rich-text__text\">Oakland CA, 94612</span></p></div></div>\n",
        "    <div>\n",
        "    <a href=\"https://maps.google.com/?q=3583+16th+St,+San+Francisco,+CA+94114,+USA&amp;ftid=0x808f7e1c89dc2e5b:0xa1d14effd3552b22\" class=\"site-location__address\" target=\"_blank\" rel=\"noopener\" data-bb-track=\"button\" data-bb-track-on=\"click\" data-bb-track-category=\"Address\" data-bb-track-action=\"Click\" data-bb-track-label=\"Header\">\n",
        "    <span>3583 16th St,</span>\n",
        "    <span> San Francisco, CA 94114</span></a>\n",
        "    </div>\n",
        "    <footer class=\"footer\" id=\"page-footer\">\n",
        "        <p>&copy; 2023 Sample Company</p>\n",
        "    </footer>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Define a regex pattern to search for \"address\" in attributes and text content\n",
        "# address_pattern = r'\\b\\d+\\s[A-Za-z\\s,]+\\b'\n",
        "\n",
        "address_pattern = re.compile(r'\\b\\d+\\s[A-Za-z\\s,]+\\b', re.IGNORECASE)\n",
        "#  same tag as above for area, address will have some more regex\n",
        "# print(address_pattern)\n",
        "# Search for the 'a' tag with an attribute or text content matching the address pattern\n",
        "matching_tag = None\n",
        "\n",
        "# for tag in soup.find_all(address_pattern):\n",
        "#     print(tag)\n",
        "#     for attr, value in tag.attrs.items():\n",
        "#         print(value)\n",
        "#         if isinstance(attr, str) and isinstance(value, str):\n",
        "#             if address_pattern.search(attr) or address_pattern.search(value):\n",
        "#                 matching_tag = tag\n",
        "#                 break\n",
        "#     if matching_tag:\n",
        "#         break\n",
        "address_elements = soup.find_all(text=re.compile(address_pattern))\n",
        "\n",
        "for element in address_elements:\n",
        "    # Find the nearest parent element with a class attribute\n",
        "    print(element)\n",
        "    parent_element = element.find_parent(attrs={\"class\": True})\n",
        "    element_class = parent_element['class'] if parent_element else None\n",
        "\n",
        "    timings = element.strip()\n",
        "\n",
        "    # Check if timings is not None before calling strip()\n",
        "    if timings is not None:\n",
        "        timings = timings.strip()\n",
        "\n",
        "    print(f\"Class Attribute: {element_class}\")\n",
        "\n",
        "# Find the parent tag of the matching 'a' tag, regardless of its tag type\n",
        "# parent_tag = matching_tag.find_parent()\n",
        "\n",
        "# Print the parent tag and its contents\n",
        "# if parent_tag:\n",
        "#     print(parent_tag)\n",
        "# else:\n",
        "#     print(f\"No parent tag found for the matching 'a' tag.\")\n",
        "\n",
        "all_text_content = []\n",
        "if matching_tag:\n",
        "    for tag in matching_tag.find_all():\n",
        "        text = tag.get_text(separator='', strip=True)\n",
        "        if text:\n",
        "          print(text)\n",
        "          all_text_content.append(text)\n",
        "\n",
        "# Join the text content and print it\n",
        "result = ', '.join(all_text_content)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfnBBn0QBIQi"
      },
      "source": [
        "## phone number\n",
        "also include email in same function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RazkHCHUBHYC",
        "outputId": "21d7d633-7529-4e10-aa66-b0e41d4f68ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['+1 (123) 123-1223', '+9 (123) 123-1223']\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Sample HTML content\n",
        "html_content = \"\"\"\n",
        "<html>\n",
        "<head>\n",
        "    <title>Sample Page</title>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"header\" id=\"top-header\">\n",
        "        <h1>Contact Us</h1>\n",
        "        <p>Phone: +1 (123) 123-1223</p>\n",
        "        <p>Email: info@example.com</p>\n",
        "    </div>\n",
        "    <div class=\"content\">\n",
        "        <p class=\"text\">Visit our site for more information.</p>\n",
        "    </div>\n",
        "    <div>\n",
        "    <a href=\"https://maps.google.com/?q=3583+16th+St,+San+Francisco,+CA+94114,+USA&amp;ftid=0x808f7e1c89dc2e5b:0xa1d14effd3552b22\" class=\"site-location__address\" target=\"_blank\" rel=\"noopener\" data-bb-track=\"button\" data-bb-track-on=\"click\" data-bb-track-category=\"Address\" data-bb-track-action=\"Click\" data-bb-track-label=\"Header\">\n",
        "    <span>3583 16th St,</span>\n",
        "    <p>Phone: +9 (123) 123-1223</p>\n",
        "    <span> San Francisco, CA 94114</span></a>\n",
        "    </div>\n",
        "    <footer class=\"footer\" id=\"page-footer\">\n",
        "        <p>&copy; 2023 Sample Company</p>\n",
        "    </footer>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "phone_pattern = r'\\+\\d{1,2}\\s?\\(\\d{3}\\)\\s?\\d{3}[-\\s]\\d{4}'\n",
        "\n",
        "def findPhone(pattern):\n",
        "  phones = []\n",
        "  pattern_elements = soup.find_all(string=re.compile(pattern))\n",
        "  for element in pattern_elements:\n",
        "      # Find the nearest parent element with a class attribute\n",
        "      parent_element = element.find_parent(attrs={\"class\": True})\n",
        "      element_class = parent_element['class'] if parent_element else None\n",
        "\n",
        "      stripped_ele = element.strip()\n",
        "\n",
        "      # Check if phone_number is not None before calling strip()\n",
        "      if stripped_ele is not None:\n",
        "          stripped_ele = stripped_ele.strip()\n",
        "\n",
        "      match = re.search(phone_pattern, stripped_ele)\n",
        "      phones.append(match.group())\n",
        "\n",
        "  return phones\n",
        "\n",
        "print(findPhone(phone_pattern))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAclW9fBHycc"
      },
      "source": [
        "## email"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMG-RVzUHw_p",
        "outputId": "4238ec61-ea0f-4c8b-83cc-94095b6ab32e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "info@example.com\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Sample HTML content\n",
        "html_content = \"\"\"\n",
        "<html>\n",
        "<head>\n",
        "    <title>Sample Page</title>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"header\" id=\"top-header\">\n",
        "        <h1>Contact Us</h1>\n",
        "        <p>Phone: +1 (123) 123-1223</p>\n",
        "        <p>Email: info@example.com</p>\n",
        "    </div>\n",
        "    <div class=\"content\">\n",
        "        <p class=\"text\">Visit our site for more information.</p>\n",
        "    </div>\n",
        "    <div>\n",
        "    <a href=\"https://maps.google.com/?q=3583+16th+St,+San+Francisco,+CA+94114,+USA&amp;ftid=0x808f7e1c89dc2e5b:0xa1d14effd3552b22\" class=\"site-location__address\" target=\"_blank\" rel=\"noopener\" data-bb-track=\"button\" data-bb-track-on=\"click\" data-bb-track-category=\"Address\" data-bb-track-action=\"Click\" data-bb-track-label=\"Header\">\n",
        "    <span>3583 16th St,</span>\n",
        "    <span> San Francisco, CA 94114</span></a>\n",
        "    </div>\n",
        "    <footer class=\"footer\" id=\"page-footer\">\n",
        "        <p>&copy; 2023 Sample Company</p>\n",
        "    </footer>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
        "\n",
        "def findEmail(pattern):\n",
        "  pattern_elements = soup.find_all(string=re.compile(pattern))\n",
        "  for element in pattern_elements:\n",
        "      # Find the nearest parent element with a class attribute\n",
        "      parent_element = element.find_parent(attrs={\"class\": True})\n",
        "      element_class = parent_element['class'] if parent_element else None\n",
        "\n",
        "      stripped_ele = element.strip()\n",
        "\n",
        "      # Check if phone_number is not None before calling strip()\n",
        "      if stripped_ele is not None:\n",
        "          stripped_ele = stripped_ele.strip()\n",
        "\n",
        "      match = re.search(pattern, stripped_ele)\n",
        "      return match.group()\n",
        "\n",
        "print(findEmail(email_pattern))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKjTJ9FSKUYN"
      },
      "source": [
        "## offers, boolean return\n",
        "pickup, group order, catering, counter, bar, delivery, reservation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YliVmEIqKit3",
        "outputId": "38f42afa-db08-4152-dec9-69272f45c2d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pickup True\n",
            "Group True\n",
            "Catering False\n",
            "Counter True\n",
            "Bar True\n",
            "Delivery True\n",
            "Reservations True\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Sample HTML content\n",
        "html_content = \"\"\"\n",
        "<html>\n",
        "<head>\n",
        "    <title>Sample Page</title>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"header\" id=\"top-header\">\n",
        "        <h1>Contact Us</h1>\n",
        "        <p>Phone: +1 (123) 123-1223</p>\n",
        "        <p>Email: info@example.com</p>\n",
        "    </div>\n",
        "    <div class=\"content\">\n",
        "        <p class=\"text\">Visit our site for more information.</p>\n",
        "        <button shape=\"Pill\" size=\"12\" kind=\"BUTTON/PRIMARY\" data-testid=\"MenuOrderMethodTogglePickup\" data-anchor-id=\"MenuOrderMethodTogglePickup\" aria-checked=\"true\" role=\"radio\" class=\"styles__StyledButtonRoot-sc-1ldytso-0 kXBxRx\"><span kind=\"BUTTON/PRIMARY\" class=\"Inset__StyledInset-sc-1phi2ey-0 ipbHIR styles__ContentWrapper-sc-1ldytso-2 hasZAr\"><span class=\"InlineChildren__StyledInlineChildren-sc-6r2tfo-0 gviwpu\"><span class=\"styles__MainContentContainer-sc-1ldytso-3 qvNNS\"><span overflow=\"truncate\" display=\"block\" class=\"styles__TextElement-sc-3qedjx-0 ciJJYj\">Pickup</span></span></span></span></button>\n",
        "\n",
        "    </div>\n",
        "    <div>\n",
        "    <a href=\"https://maps.google.com/?q=3583+16th+St,+San+Francisco,+CA+94114,+USA&amp;ftid=0x808f7e1c89dc2e5b:0xa1d14effd3552b22\" class=\"site-location__address\" target=\"_blank\" rel=\"noopener\" data-bb-track=\"button\" data-bb-track-on=\"click\" data-bb-track-category=\"Address\" data-bb-track-action=\"Click\" data-bb-track-label=\"Header\">\n",
        "    <span>3583 16th St,</span>\n",
        "<button shape=\"Pill\" size=\"12\" kind=\"BUTTON/PRIMARY\" data-testid=\"MenuOrderMethodToggleDelivery\" data-anchor-id=\"MenuOrderMethodToggleDelivery\" aria-checked=\"false\" role=\"radio\" class=\"styles__StyledButtonRoot-sc-1ldytso-0 kJHgFL\"><span kind=\"BUTTON/PRIMARY\" class=\"Inset__StyledInset-sc-1phi2ey-0 ipbHIR styles__ContentWrapper-sc-1ldytso-2 hasZAr\"><span class=\"InlineChildren__StyledInlineChildren-sc-6r2tfo-0 gviwpu\"><span class=\"styles__MainContentContainer-sc-1ldytso-3 qvNNS\"><span overflow=\"truncate\" display=\"block\" class=\"styles__TextElement-sc-3qedjx-0 ciJJYj\">Delivery</span></span></span></span></button>\n",
        "    <span> Group orders</span></a>\n",
        "    <span> San Francisco, CA 94114</span></a>\n",
        "    <span> bar</span></a><span> San Francisco, CA 94114</span></a><span> San reservations Francisco, CA 94114</span></a>\n",
        "    <span> San Francisco,delivery CA 94114</span></a>\n",
        "    <span> Counter</span></a>\n",
        "    </div>\n",
        "    <footer class=\"footer\" id=\"page-footer\">\n",
        "        <p>&copy; 2023 Sample Company</p>\n",
        "    </footer>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "keywords = [\"Pickup\", \"Group\", \"Catering\", \"Counter\", \"Bar\", \"Delivery\", \"Reservations\"]\n",
        "\n",
        "\n",
        "def findPattern(keyword):\n",
        "  if soup.find_all(string=re.compile(r'\\b(?:' + keyword + r')\\b', re.IGNORECASE)):\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "for keyword in keywords:\n",
        "  print(keyword, findPattern(keyword))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njsP2sC4VJud"
      },
      "source": [
        "## year established"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPoImCYjVJaH",
        "outputId": "62e71349-224b-475a-c772-8e8eb06a5c8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1999, 1960]\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# Sample HTML content\n",
        "html_content = \"\"\"\n",
        "<html>\n",
        "<head>\n",
        "    <title>Sample Page</title>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"header\" id=\"top-header\">\n",
        "        <h1>Contact Us</h1>\n",
        "        <p> 1999</p>\n",
        "        <p>Email: info@example.com</p>\n",
        "        <p>Phone: +1 (123) 123-1223</p>\n",
        "    </div>\n",
        "    <div class=\"content\">\n",
        "        <p class=\"text\">Visit our site for more information.</p>\n",
        "        <button shape=\"Pill\" size=\"12\" kind=\"BUTTON/PRIMARY\" data-testid=\"MenuOrderMethodTogglePickup\" data-anchor-id=\"MenuOrderMethodTogglePickup\" aria-checked=\"true\" role=\"radio\" class=\"styles__StyledButtonRoot-sc-1ldytso-0 kXBxRx\"><span kind=\"BUTTON/PRIMARY\" class=\"Inset__StyledInset-sc-1phi2ey-0 ipbHIR styles__ContentWrapper-sc-1ldytso-2 hasZAr\"><span class=\"InlineChildren__StyledInlineChildren-sc-6r2tfo-0 gviwpu\"><span class=\"styles__MainContentContainer-sc-1ldytso-3 qvNNS\"><span overflow=\"truncate\" display=\"block\" class=\"styles__TextElement-sc-3qedjx-0 ciJJYj\">Pickup</span></span></span></span></button>\n",
        "\n",
        "    </div>\n",
        "\n",
        "    <div>\n",
        "    <p style=\"text-align: left; color: green;\">We opened as âClub Sinaloaâ in 1960 as a meals.</p>\n",
        "    <a href=\"https://maps.google.com/?q=3583+16th+St,+San+Francisco,+CA+94114,+USA&amp;ftid=0x808f7e1c89dc2e5b:0xa1d14effd3552b22\" class=\"site-location__address\" target=\"_blank\" rel=\"noopener\" data-bb-track=\"button\" data-bb-track-on=\"click\" data-bb-track-category=\"Address\" data-bb-track-action=\"Click\" data-bb-track-label=\"Header\">\n",
        "    <span>3583 16th St,</span>\n",
        "<button shape=\"Pill\" size=\"12\" kind=\"BUTTON/PRIMARY\" data-testid=\"MenuOrderMethodToggleDelivery\" data-anchor-id=\"MenuOrderMethodToggleDelivery\" aria-checked=\"false\" role=\"radio\" class=\"styles__StyledButtonRoot-sc-1ldytso-0 kJHgFL\"><span kind=\"BUTTON/PRIMARY\" class=\"Inset__StyledInset-sc-1phi2ey-0 ipbHIR styles__ContentWrapper-sc-1ldytso-2 hasZAr\"><span class=\"InlineChildren__StyledInlineChildren-sc-6r2tfo-0 gviwpu\"><span class=\"styles__MainContentContainer-sc-1ldytso-3 qvNNS\"><span overflow=\"truncate\" display=\"block\" class=\"styles__TextElement-sc-3qedjx-0 ciJJYj\">Delivery</span></span></span></span></button>\n",
        "    <span> Group orders</span></a>\n",
        "    <span> San Francisco, CA 94114</span></a>\n",
        "    <span> bar</span></a><span> San Francisco, CA 94114</span></a><span> San reservations Francisco, CA 94114</span></a>\n",
        "    <span> San Francisco,delivery CA 94114</span></a>\n",
        "    <span> Counter</span></a>\n",
        "    <p>Phone: +9 (123) 123-1223</p>\n",
        "    </div>\n",
        "    <footer class=\"footer\" id=\"page-footer\">\n",
        "        <p>&copy; 2023 Sample Company</p>\n",
        "    </footer>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "year_pattern = r'\\b\\d{4}\\b'\n",
        "# phone_pattern = r'\\+\\d{1,2}\\s?\\(\\d{3}\\)\\s?\\d{3}[-\\s]\\d{4}'\n",
        "\n",
        "def findYear(year_pattern):\n",
        "  pattern_elements = soup.find_all(string=re.compile(year_pattern))\n",
        "  possible_year = []\n",
        "  for element in pattern_elements:\n",
        "      parent_element = element.find_parent(attrs={\"class\": True})\n",
        "      element_class = parent_element['class'] if parent_element else None\n",
        "\n",
        "      stripped_ele = element.strip()\n",
        "      possible_year.append(stripped_ele)\n",
        "  all_phones = findPhone(phone_pattern)\n",
        "  possible_year = [ele for ele in possible_year if not any(phn in ele for phn in all_phones)]\n",
        "  possible_year = [year for year in possible_year if 'Â©' not in year]\n",
        "\n",
        "  final_possible_years = []\n",
        "  for year in possible_year:\n",
        "    match = re.search(year_pattern, year)\n",
        "    final_possible_years.append(int(match.group()))\n",
        "  for yr in final_possible_years:\n",
        "    if(yr > datetime.now().year):\n",
        "      final_possible_years.remove(yr)\n",
        "  return final_possible_years\n",
        "\n",
        "print(findYear(year_pattern))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFccoWcz2bEe"
      },
      "source": [
        "## time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITOS66QM2eYQ",
        "outputId": "9312fbce-84a3-493b-83fc-c79595e59ac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â»\n",
            "Sunday thru Thursday\n",
            "      11:00amÂ  to 9:00pm\n",
            "Friday and Saturday\n",
            "      11:00am to 10:00pm\n",
            "Last orders taken 30-minutes before closing\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Sample HTML content\n",
        "html_content = \"\"\"\n",
        "<html>\n",
        "<head>\n",
        "    <title>Sample Page</title>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"header\" id=\"top-header\">\n",
        "        <h1>Contact Us</h1>\n",
        "        <p>Phone: +1 (123) 123-1223</p>\n",
        "        <p>Email: info@example.com</p>\n",
        "    </div>\n",
        "    <div class=\"content\">\n",
        "        <p class=\"text\">Visit our site for more information.</p>\n",
        "    </div>\n",
        "    <div>\n",
        "    <div id=\"text-3\" class=\"widget-odd widget-last widget-3 centered widget widget--menu widget_text\">\n",
        "      <h4 class=\"widget__title widget--menu__title\">Business Hours</h4>\n",
        "      <div class=\"textwidget\"><div class=\"pixcode  pixcode--separator  separator separator--flower\">â»</div>\n",
        "      <p>Sunday thru Thursday<br>\n",
        "      11:00am&nbsp; to 9:00pm</p>\n",
        "      <p>Friday and Saturday<br>\n",
        "      11:00am to 10:00pm</p>\n",
        "      <h4><strong style=\"color: #ed1c24;\">Last orders taken 30-minutes before closing</strong></h4>\n",
        "    </div>\n",
        "\t\t</div>\n",
        "    </div>\n",
        "    <footer class=\"footer\" id=\"page-footer\">\n",
        "        <p>&copy; 2023 Sample Company</p>\n",
        "    </footer>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "time_pattern = r'\\b\\d{1,2}(?::\\d{2})?\\s*[APap][Mm]\\b'\n",
        "\n",
        "def findTime(pattern):\n",
        "  pattern_elements = soup.find_all(string=re.compile(pattern, re.IGNORECASE))\n",
        "  time_slots=[]\n",
        "  for element in pattern_elements:\n",
        "      # Find the nearest parent element with a class attribute\n",
        "      parent_element = element.find_parent(attrs={\"class\": True})\n",
        "      # print(parent_element.get_text())\n",
        "  return parent_element.get_text()\n",
        "\n",
        "print(findTime(t_pattern))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrLA7-J48Gqv"
      },
      "source": [
        "## about, menu --> only urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZifsYZCy8FDW",
        "outputId": "c51db77d-b312-48dc-ce0b-b9eb57449f6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "About\n",
            "<h4 class=\"widget__title widget--menu__title\">About</h4>\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Sample HTML content\n",
        "html_content = \"\"\"\n",
        "<html>\n",
        "<head>\n",
        "    <title>Sample Page</title>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"header\" id=\"top-header\">\n",
        "        <h1>Contact Us</h1>\n",
        "        <p>Phone: +1 (123) 123-1223</p>\n",
        "        <p>Email: info@example.com</p>\n",
        "    </div>\n",
        "    <div class=\"content\">\n",
        "        <p class=\"text\">Visit our site for more information.</p>\n",
        "    </div>\n",
        "    <div>\n",
        "    <div id=\"text-3\" class=\"widget-odd widget-last widget-3 centered widget widget--menu widget_text\">\n",
        "      <h4 class=\"widget__title widget--menu__title\">Business Hours</h4>\n",
        "      <div class=\"textwidget\"><div class=\"pixcode  pixcode--separator  separator separator--flower\">â»</div>\n",
        "      <p>Sunday thru Thursday<br>\n",
        "      11:00am&nbsp; to 9:00pm</p>\n",
        "      <p>Friday and Saturday<br>\n",
        "      11:00am to 10:00pm</p>\n",
        "      <h4><strong style=\"color: #ed1c24;\">Last orders taken 30-minutes before closing</strong></h4>\n",
        "    </div>\n",
        "    <div id=\"text-2\" class=\"widget-odd widget-first widget-1 centered widget widget--menu widget_text\"><h4 class=\"widget__title widget--menu__title\">About</h4>\t\t\t<div class=\"textwidget\"><div class=\"pixcode  pixcode--separator  separator separator--flower\">â»</div>\n",
        "\n",
        "    <h2><strong style=\"color: red;\">SINALOA CAFE</strong></h2>\n",
        "    <p style=\"text-align: left; color: green;\">We opened as âClub Sinaloaâ in 1960 as a dance club and bar. Adolfo &amp; Mary Pena. never dreamed it would turn into a family tradition lasting over 50 years. As it was, patrons stayed late and ate tacos and burritos before leaving. which encouraged to expand the kitchen, dining room and menu.<br>\n",
        "    Weâve always used the freshest produce and the highest quality of meats to prepare our famous meals.</p>\n",
        "    </div>\n",
        "\t\t</div>\n",
        "\t\t</div>\n",
        "    </div>\n",
        "    <footer class=\"footer\" id=\"page-footer\">\n",
        "        <p>&copy; 2023 Sample Company</p>\n",
        "    </footer>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "t_pattern = r'\\babout\\b'\n",
        "\n",
        "def findEmail(pattern):\n",
        "  pattern_elements = soup.find_all(string=re.compile(pattern, re.IGNORECASE))\n",
        "  for element in pattern_elements:\n",
        "      # Find the nearest parent element with a class attribute\n",
        "      print(element)\n",
        "      parent_element = element.find_parent(attrs={\"class\": True})\n",
        "      print(parent_element)\n",
        "\n",
        "print(findEmail(t_pattern))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re  \n",
        "import time  \n",
        "from requests.exceptions import Timeout, RequestException  # Import the Timeout exception for links which takes too much time to load\n",
        "\n",
        "# Function to categorize links on a webpage\n",
        "# time function \n",
        "time_pattern = r'\\b\\d{1,2}(?::\\d{2})?\\s*[APap][Mm]\\b'\n",
        "phone_pattern = r'\\+\\d{1,2}\\s?\\(\\d{3}\\)\\s?\\d{3}[-\\s]\\d{4}'\n",
        "year_pattern = r'\\b\\d{4}\\b'\n",
        "keywords = [\"Pickup\", \"Group\", \"Catering\", \"Counter\", \"Bar\", \"Delivery\", \"Reservations\"]\n",
        "\n",
        "def findTime(pattern):\n",
        "  pattern_elements = soup.find_all(string=re.compile(pattern, re.IGNORECASE))\n",
        "  time_slots=[]\n",
        "  for element in pattern_elements:\n",
        "      # Find the nearest parent element with a class attribute\n",
        "      parent_element = element.find_parent(attrs={\"class\": True})\n",
        "      # print(parent_element.get_text())\n",
        "  return parent_element.get_text()\n",
        "\n",
        "\n",
        "\n",
        "#year function\n",
        "\n",
        "# phone_pattern = r'\\+\\d{1,2}\\s?\\(\\d{3}\\)\\s?\\d{3}[-\\s]\\d{4}'\n",
        "\n",
        "def findYear(year_pattern):\n",
        "  pattern_elements = soup.find_all(string=re.compile(year_pattern))\n",
        "  possible_year = []\n",
        "  for element in pattern_elements:\n",
        "    \n",
        "\n",
        "      stripped_ele = element.strip()\n",
        "      possible_year.append(stripped_ele)\n",
        "  all_phones = findPhone(phone_pattern)\n",
        "  if all_phones:\n",
        "    possible_year = [ele for ele in possible_year if not any(phn in ele for phn in all_phones)]\n",
        "    possible_year = [year for year in possible_year if 'Â©' not in year]\n",
        "\n",
        "  final_possible_years = []\n",
        "  for year in possible_year:\n",
        "    match = re.search(year_pattern, year)\n",
        "    final_possible_years.append(int(match.group()))\n",
        "  for yr in final_possible_years:\n",
        "    if(yr > datetime.now().year):\n",
        "      final_possible_years.remove(yr)\n",
        "\n",
        "\n",
        "  return final_possible_years\n",
        "\n",
        "\n",
        "\n",
        "# all boolean in one (eg.has_delivery,has_catering.......)\n",
        "# it will return list so handle carefully\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def findPattern(keyword):\n",
        "  if soup.find_all(string=re.compile(r'\\b(?:' + keyword + r')\\b', re.IGNORECASE)):\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "for keyword in keywords:\n",
        "  print(keyword, findPattern(keyword))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# email \n",
        "\n",
        "email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
        "\n",
        "def findEmail(pattern):\n",
        "  pattern_elements = soup.find_all(string=re.compile(pattern))\n",
        "  for element in pattern_elements:\n",
        "      # Find the nearest parent element with a class attribute\n",
        "      parent_element = element.find_parent(attrs={\"class\": True})\n",
        "      element_class = parent_element['class'] if parent_element else None\n",
        "\n",
        "      stripped_ele = element.strip()\n",
        "\n",
        "      # Check if phone_number is not None before calling strip()\n",
        "      if stripped_ele is not None:\n",
        "          stripped_ele = stripped_ele.strip()\n",
        "\n",
        "      match = re.search(pattern, stripped_ele)\n",
        "      return match.group()\n",
        "\n",
        "\n",
        "\n",
        "# mobile number , it will return list so handle with\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def findPhone(pattern):\n",
        "  phones = []\n",
        "  pattern_elements = soup.find_all(string=re.compile(pattern))\n",
        "  for element in pattern_elements:\n",
        "      # Find the nearest parent element with a class attribute\n",
        "      parent_element = element.find_parent(attrs={\"class\": True})\n",
        "      element_class = parent_element['class'] if parent_element else None\n",
        "\n",
        "      stripped_ele = element.strip()\n",
        "\n",
        "      # Check if phone_number is not None before calling strip()\n",
        "      if stripped_ele is not None:\n",
        "          stripped_ele = stripped_ele.strip()\n",
        "\n",
        "      match = re.search(phone_pattern, stripped_ele)\n",
        "      phones.append(match.group())\n",
        "\n",
        "  return phones\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def categorize_links(links):\n",
        "    if links is None:\n",
        "        return [] \n",
        "    maps_link = \"\"\n",
        "    instagram_link = \"\"\n",
        "    facebook_link = \"\"\n",
        "    twitter_link = \"\"\n",
        "    other_links = []\n",
        "\n",
        "    for link in links:\n",
        "        if \"maps.google.com\" in link:\n",
        "            maps_link = link\n",
        "        elif \"www.instagram.com\" in link:\n",
        "            instagram_link = link\n",
        "        elif \"www.facebook.com\" in link:\n",
        "            facebook_link = link\n",
        "        elif \"twitter.com\" in link:\n",
        "            twitter_link = link\n",
        "        else:\n",
        "            other_links.append(link)\n",
        "\n",
        "    return [maps_link, instagram_link, facebook_link, twitter_link] + other_links\n",
        "\n",
        "\n",
        "def extract_links_with_error_handling(url, index, visited_links=None, depth=0, max_depth=2):\n",
        "    if visited_links is None:\n",
        "        visited_links = set()\n",
        "\n",
        "    retries = 3  # Number of retries before giving up\n",
        "    retry_delay = 2  # Delay between retries in seconds\n",
        "    unique_links = set()\n",
        "\n",
        "    try:\n",
        "        if url not in visited_links and depth <= max_depth:\n",
        "            visited_links.add(url)\n",
        "            print(f'Crawling {url}')\n",
        "            response = requests.get(url, timeout=(5, 5))  # Set a timeout of 10 seconds for both connect and read\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            email=findPattern(email_pattern)\n",
        "            phone_number=findPhone(phone_pattern)\n",
        "            timing=findTime(time_pattern)\n",
        "            year_established=findYear(year_pattern)\n",
        "            print(email,phone_number,timing,year_established)\n",
        "            for key in keywords:\n",
        "                print(key,findPattern(key))\n",
        "    \n",
        "            links = [a['href'] for a in soup.find_all('a', href=True)]\n",
        "            valid_links = [link for link in links if re.match(r'^https?://', link)]\n",
        "\n",
        "            for link in valid_links:\n",
        "                if not any(social in link for social in [\"whatsapp\", \"insta\", \"facebook\", \"twitter\"]):\n",
        "                    unique_links.add(link)\n",
        "\n",
        "            for link in unique_links.copy():\n",
        "                unique_links |= extract_links_with_error_handling(link, index, visited_links, depth + 1, max_depth)\n",
        "\n",
        "    except Timeout:\n",
        "        print(f\"Timeout occurred for index {index} and URL {url}. Retrying...\")\n",
        "        time.sleep(retry_delay)\n",
        "    except RequestException as e:\n",
        "        print(f\"RequestException occurred for index {index} and URL {url}: {str(e)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting links from index {index} for URL {url}: {str(e)}\")\n",
        "\n",
        "    return unique_links\n",
        "\n",
        "\n",
        "# Initialize an empty dictionary to store the data\n",
        "link_data = {}\n",
        "\n",
        "# Read the CSV file containing links\n",
        "csv_filename = 'outfile.csv'  # Change this to your CSV file name\n",
        "with open(csv_filename, 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    \n",
        "    # Iterate over each row in the CSV file\n",
        "    for index, row in enumerate(reader):\n",
        "        if len(row) < 2:\n",
        "            continue  # Skip rows with less than 2 columns\n",
        "        link_index = row[0]\n",
        "        link = row[1]\n",
        "        if(link == ''):\n",
        "            continue\n",
        "        print(' ---------------------------going for-----------------------------\\n ', link_index, ' : ', link)\n",
        "        visited_links = set()  \n",
        "        max_depth = 1\n",
        "        link_data[link] = extract_links_with_error_handling(link, link_index, visited_links, depth=0, max_depth=max_depth)\n",
        "\n",
        "        time.sleep(1) \n",
        "\n",
        "# Categorize links after extracting all links\n",
        "for key, values in link_data.items():\n",
        "    link_data[key] = categorize_links(values)\n",
        "\n",
        "# Print the resulting dictionary and save it to an output file\n",
        "output_filename = 'deepcrawl/links_large.csv'\n",
        "with open(output_filename, 'w', newline='') as output_csv:\n",
        "    writer = csv.writer(output_csv)\n",
        "    for key, values in link_data.items():\n",
        "        print(f\"Link: {key}\")\n",
        "        print(\"Links found on the page:\")\n",
        "        writer.writerow([key] + values)\n",
        "        for value in values:\n",
        "            print(f\" - {value}\")\n",
        "        print()\n",
        "\n",
        "print(f\"Data saved to {output_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPzCV4tIZnhV"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lco5BluZpot"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
